{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# FULL 128x128 DDPM TRAINING PIPELINE\n# Features: Mixed Precision, Gradient Accumulation, EMA, and Robust Data Loading.\n\nprint(\"ðŸ”¥ Script started â€” loading imports...\")\n\n# ---------------- CRITICAL FIX FOR DATASET ----------------\n# Standard PIL library often crashes on truncated (partially downloaded) images.\n# This setting forces PIL to load them anyway, filling missing data with grey/black\n# preventing the entire training loop from crashing due to one bad file.\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport os\nimport random\nimport math\nimport copy\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# ---------------- CONFIGURATION ----------------\nDATASET_PATH = \"/kaggle/input/cartoon-classification/cartoon_classification/TRAIN\"\nIMG_SIZE = 128            # Higher res = better detail but exponentially more VRAM usage\nBASE_CH = 32              # Base network width. 32->64->128->256 channels.\nBATCH_SIZE = 2            # Kept tiny (2) to prevent \"CUDA Out Of Memory\" on limited GPUs.\nACCUM_STEPS = 4           # GRADIENT ACCUMULATION: We only update weights every 4 steps.\n                          # Effective Batch Size = 2 * 4 = 8 images.\nEPOCHS = 12\nLR = 2e-4                 # Standard learning rate for AdamW optimizer\nNUM_WORKERS = 4           # Uses 4 CPU cores to preload images (prevents GPU waiting for data)\nPIN_MEMORY = True         # Pins RAM to speed up transfer of data from CPU -> GPU\nTIMESTEPS = 1000          # Standard diffusion: noise is added over 1000 steps\nSUBSET_SIZE = None        # Set to an integer (e.g., 100) to debug on a small dataset quickly\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nCHECKPOINT_DIR = \"checkpoints\"\nCHECKPOINT_FREQ = 1       # Save weights every epoch\nSAMPLES_TO_SAVE = 4       # How many test images to generate at the end\nSEED = 42\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(f\"Config: Device={DEVICE} | Size={IMG_SIZE} | Batch={BATCH_SIZE}x{ACCUM_STEPS}\")\n\n# ---------------- DATASET ----------------\nclass CartoonColorizationDataset(Dataset):\n    def __init__(self, root, img_size=IMG_SIZE, subset=SUBSET_SIZE):\n        self.paths = []\n        # Walk through directories to find all image files\n        for r, d, f in os.walk(root):\n            for x in f:\n                if x.lower().endswith((\"jpg\",\"jpeg\",\"png\")):\n                    self.paths.append(os.path.join(r, x))\n        \n        self.paths.sort() # Sorting ensures reproducibility (same order every run)\n        if subset:\n            self.paths = self.paths[:subset]\n            \n        # Transform Pipeline: Resizes image and converts to PyTorch Tensor (0-1 range)\n        self.tf = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()\n        ])\n        self.gray = transforms.Grayscale() # Used to create the input condition\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        \n        # ERROR HANDLING:\n        # If an image is corrupt, instead of crashing the training, \n        # we generate a blank grey image. This keeps the training loop alive.\n        try:\n            img = Image.open(p).convert('RGB')\n        except Exception:\n            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color=(128,128,128))\n\n        try:\n            y = self.tf(img)               # This is the GROUND TRUTH (Color) we want to predict\n            x = self.tf(self.gray(img))    # This is the CONDITION (Black & White) we give the model\n        except Exception:\n            # Fallback if transformation fails\n            y = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n            x = torch.zeros(1, IMG_SIZE, IMG_SIZE)\n\n        return x, y\n\n# ---------------- NOISE SCHEDULE ----------------\n# Diffusion works by slowly adding noise. We need to pre-calculate exactly how much\n# noise to add at every timestep (from 0 to 1000).\n\ndef linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n    # Creates a linear ramp of noise variance\n    return torch.linspace(beta_start, beta_end, timesteps)\n\nclass NoiseSchedule:\n    def __init__(self, timesteps=TIMESTEPS, device='cpu'):\n        self.timesteps = timesteps\n        betas = linear_beta_schedule(timesteps).to(device)\n        alphas = 1.0 - betas\n        \n        # Alphas Cumulative Product: Represents how much \"Original Signal\" is left.\n        # At t=0, this is 1.0 (pure signal). At t=1000, it approaches 0 (pure noise).\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), alphas_cumprod[:-1]], dim=0)\n\n        self.betas = betas\n        self.alphas = alphas\n        self.alphas_cumprod = alphas_cumprod\n        self.alphas_cumprod_prev = alphas_cumprod_prev\n\n        # Pre-calculate square roots for the forward diffusion formula:\n        # x_t = sqrt(alpha_cumprod) * x_0  +  sqrt(1 - alpha_cumprod) * noise\n        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Forward Process: Adds noise to an image 'x_start' to timestep 't'.\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start) # Generate Gaussian noise\n        \n        # Gather the specific alpha values for the requested timesteps (t)\n        # .view(-1, 1, 1, 1) reshapes the scalars to broadcast over image dimensions [B, C, H, W]\n        a = self.sqrt_alphas_cumprod[t].view(-1,1,1,1)\n        b = self.sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n        \n        return a * x_start + b * noise, noise\n\n# ---------------- MODEL (UNET) ----------------\n# The UNet predicts the NOISE that was added to the image.\n\nclass SinusoidalPositionEmb(nn.Module):\n    \"\"\"\n    Encodes the timestep 't' into a vector. \n    This allows the UNet to know if it's looking at a slightly noisy image (t=10)\n    or a completely destroyed static image (t=900).\n    \"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        # Uses sine and cosine frequencies (similar to Transformer positional encodings)\n        half = self.dim // 2\n        emb = math.log(10000) / (half - 1)\n        emb = torch.exp(torch.arange(half, device=t.device) * -emb)\n        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n        if self.dim % 2 == 1:\n            emb = torch.cat([emb, torch.zeros(t.size(0),1,device=t.device)], dim=1)\n        return emb\n\nclass ConvBlock(nn.Module):\n    \"\"\"\n    Standard building block: Conv2d -> GroupNorm -> SiLU (Swish) -> Conv2d -> GroupNorm -> SiLU\n    \"\"\"\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            # We use GroupNorm instead of BatchNorm because our Batch Size is very small (2).\n            # BatchNorm becomes unstable with small batches; GroupNorm is robust.\n            nn.GroupNorm(8, out_ch) if out_ch>=8 else nn.BatchNorm2d(out_ch),\n            nn.SiLU(), # SiLU (Swish) activation works better for diffusion than ReLU\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.GroupNorm(8, out_ch) if out_ch>=8 else nn.BatchNorm2d(out_ch),\n            nn.SiLU()\n        )\n        # Residual connection: If input/output channels differ, project input to match output.\n        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch!=out_ch else nn.Identity()\n\n    def forward(self, x, t_emb=None):\n        h = self.net(x)\n        if t_emb is not None:\n            # Inject time embedding info into the features\n            te = t_emb.unsqueeze(-1).unsqueeze(-1)\n            # Project time embedding to match feature channel count\n            if te.shape[1] != h.shape[1]:\n                proj = nn.Linear(te.shape[1], h.shape[1]).to(te.device)\n                te = proj(t_emb).unsqueeze(-1).unsqueeze(-1)\n            h = h + te # Add time info to the image features\n        return h + self.res_conv(x) # Add residual connection\n\nclass MediumUNet(nn.Module):\n    def __init__(self, in_ch=4, base_ch=BASE_CH, time_emb_dim=128):\n        super().__init__()\n        # Time Embedding MLP (Multi-Layer Perceptron)\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmb(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim*4),\n            nn.SiLU(),\n            nn.Linear(time_emb_dim*4, time_emb_dim)\n        )\n\n        # Encoder (Downsampling): Reduces image size, increases channels (learning features)\n        self.enc1 = ConvBlock(in_ch, base_ch)\n        self.down1 = nn.Conv2d(base_ch, base_ch*2, 4, stride=2, padding=1)\n        self.enc2 = ConvBlock(base_ch*2, base_ch*2)\n        self.down2 = nn.Conv2d(base_ch*2, base_ch*4, 4, stride=2, padding=1)\n        self.enc3 = ConvBlock(base_ch*4, base_ch*4)\n        self.down3 = nn.Conv2d(base_ch*4, base_ch*8, 4, stride=2, padding=1)\n\n        # Bottleneck: The deepest part of the network\n        self.bot1 = ConvBlock(base_ch*8, base_ch*8)\n        self.bot2 = ConvBlock(base_ch*8, base_ch*8)\n\n        # Decoder (Upsampling): Increases image size, reduces channels (reconstructing details)\n        # We use Transpose Convolutions to learn how to upscale.\n        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 4, stride=2, padding=1)\n        self.dec3 = ConvBlock(base_ch*8, base_ch*4)\n        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 4, stride=2, padding=1)\n        self.dec2 = ConvBlock(base_ch*4, base_ch*2)\n        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 4, stride=2, padding=1)\n        self.dec1 = ConvBlock(base_ch*2, base_ch)\n\n        # Final projection to 3 RGB channels (Predicting the noise)\n        self.out = nn.Conv2d(base_ch, 3, 3, padding=1)\n\n    def forward(self, x_gray, y_noisy, t):\n        # 1. Process Time Embedding\n        t_emb = self.time_mlp(t)\n        \n        # 2. CONDITIONING STEP (CRITICAL):\n        # We concatenate the Noisy Image (3 channels) with the Grayscale Hint (1 channel)\n        # Result = 4 channels. This tells the model: \"Given this grayscale shape, denoise this RGB static.\"\n        x = torch.cat([y_noisy, x_gray], dim=1)\n\n        # 3. Downsample (Store skip connections e1, e2, e3)\n        e1 = self.enc1(x, t_emb)\n        d1 = self.enc2(self.down1(e1), t_emb)\n        e2 = d1\n        d2 = self.enc3(self.down2(e2), t_emb)\n        e3 = d2\n        d3 = self.bot1(self.down3(e3), t_emb)\n        d3 = self.bot2(d3, t_emb)\n\n        # 4. Upsample (Concatenate with skip connections to preserve spatial details)\n        u3 = self.up3(d3)\n        c3 = torch.cat([u3, e3], dim=1) # Skip connection\n        u3 = self.dec3(c3, t_emb)\n\n        u2 = self.up2(u3)\n        c2 = torch.cat([u2, e2], dim=1)\n        u2 = self.dec2(c2, t_emb)\n\n        u1 = self.up1(u2)\n        c1 = torch.cat([u1, e1], dim=1)\n        u1 = self.dec1(c1, t_emb)\n\n        return self.out(u1)\n\n# ---------------- EMA & SAMPLER ----------------\nclass EMA:\n    \"\"\"\n    Exponential Moving Average.\n    We maintain a 'shadow' copy of the model weights that updates slowly.\n    This creates smoother results and prevents the model from jittering at the very end of training.\n    \"\"\"\n    def __init__(self, model, decay=0.9999):\n        self.ema = copy.deepcopy(model).eval()\n        self.decay = decay\n        self.num_updates = 0\n\n    def update(self, model):\n        self.num_updates += 1\n        # Dynamic decay adjustment (optional, but helps early training)\n        decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n        msd = model.state_dict()\n        for k, v in self.ema.state_dict().items():\n            if v.dtype.is_floating_point:\n                # v_new = decay * v_old + (1-decay) * current_model_weight\n                v *= decay\n                v += (1 - decay) * msd[k].detach()\n            else:\n                v.copy_(msd[k])\n\n@torch.no_grad()\ndef ddim_sample(model, x_gray, schedule, steps=50, eta=0.0, use_ema=False, ema_obj=None):\n    \"\"\"\n    DDIM (Denoising Diffusion Implicit Models) Sampler.\n    Unlike standard DDPM (which needs 1000 steps), DDIM can generate high-quality images \n    in just 50 steps by skipping timesteps deterministically.\n    \"\"\"\n    device = x_gray.device\n    B = x_gray.size(0)\n    T = schedule.timesteps\n    # Create a list of timesteps to visit (e.g., 999, 979, 959... 0)\n    times = torch.linspace(T-1, 0, steps).long().to(device)\n\n    # Start with pure random noise\n    y = torch.randn(B,3,IMG_SIZE,IMG_SIZE, device=device)\n\n    # Swap in EMA weights for inference if requested (gives better quality)\n    if use_ema and ema_obj:\n        model_backup = copy.deepcopy(model.state_dict())\n        model.load_state_dict(ema_obj.ema.state_dict())\n\n    # The Reverse Diffusion Loop\n    for i, t in enumerate(times):\n        t_batch = torch.full((B,), int(t.item()), dtype=torch.long, device=device)\n        \n        # Model predicts the noise in the current image 'y'\n        eps_pred = model(x_gray, y, t_batch)\n\n        alpha_t = schedule.alphas_cumprod[t].to(device)\n        alpha_prev = schedule.alphas_cumprod_prev[t].to(device)\n        sqrt_alpha_t = torch.sqrt(alpha_t)\n        \n        # 1. Predict what the clean image (x0) looks like\n        x0_pred = (y - torch.sqrt(1 - alpha_t) * eps_pred) / (sqrt_alpha_t + 1e-8)\n        \n        # 2. Point towards the direction of the next timestep\n        dir_xt = torch.sqrt(1 - alpha_prev) * eps_pred\n        \n        # 3. Combine to get the image at the previous timestep\n        y = torch.sqrt(alpha_prev) * x0_pred + dir_xt\n\n    # Restore original weights if EMA was used\n    if use_ema and ema_obj:\n        model.load_state_dict(model_backup)\n\n    return y.clamp(0,1) # Ensure pixel values are valid image colors\n\n# ---------------- TRAINING LOOP ----------------\ndef train(model, schedule, loader, opt, scaler, ema_obj=None):\n    model.train()\n    global_step = 0\n    \n    for epoch in range(EPOCHS):\n        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        running_loss = 0.0\n        opt.zero_grad()\n        \n        for step, (x_gray, y) in enumerate(pbar):\n            try:\n                # non_blocking=True allows GPU to compute while CPU loads next batch\n                x_gray = x_gray.to(DEVICE, non_blocking=PIN_MEMORY)\n                y = y.to(DEVICE, non_blocking=PIN_MEMORY)\n                \n                B = y.size(0)\n                \n                # Pick random timesteps for each image in batch (0 to 1000)\n                t = torch.randint(0, schedule.timesteps, (B,), device=DEVICE)\n                \n                # Add noise to the clean images (Forward Diffusion)\n                y_noisy, noise = schedule.q_sample(y, t)\n\n                # MIXED PRECISION (Autocast):\n                # Runs math in float16 (half precision) where possible to save VRAM and speed up training.\n                with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n                    # Model tries to predict the noise we just added\n                    pred = model(x_gray, y_noisy, t)\n                    # Loss = Mean Squared Error between Predicted Noise and Actual Noise\n                    loss = ((pred - noise)**2).mean() / ACCUM_STEPS\n\n                # SCALER: Helps standard gradients flow through float16 math without vanishing\n                scaler.scale(loss).backward()\n\n                # GRADIENT ACCUMULATION:\n                # We only step the optimizer every ACCUM_STEPS (4).\n                # This simulates a larger batch size than our GPU memory can actually hold.\n                if (step + 1) % ACCUM_STEPS == 0:\n                    scaler.step(opt)\n                    scaler.update()\n                    opt.zero_grad()\n                    if ema_obj:\n                        ema_obj.update(model)\n\n                running_loss += float(loss.item()) * ACCUM_STEPS\n                global_step += 1\n                pbar.set_postfix({'loss': running_loss / (global_step)})\n\n            except RuntimeError as e:\n                # OOM (Out Of Memory) Protection:\n                # If the GPU fills up, we clear the cache and skip the batch instead of crashing.\n                if 'out of memory' in str(e):\n                    print('OOM: Clearing cache...')\n                    torch.cuda.empty_cache()\n                    opt.zero_grad()\n                    continue\n                else:\n                    raise\n\n        # Save checkpoints\n        if (epoch + 1) % CHECKPOINT_FREQ == 0:\n            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'model_epoch{epoch+1}.pth'))\n            if ema_obj:\n                torch.save(ema_obj.ema.state_dict(), os.path.join(CHECKPOINT_DIR, f'ema_epoch{epoch+1}.pth'))\n\n    print('Training complete.')\n\n# ---------------- MAIN ----------------\ndef main():\n    dataset = CartoonColorizationDataset(DATASET_PATH, img_size=IMG_SIZE, subset=SUBSET_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n    print(f'Total images: {len(dataset)}')\n\n    schedule = NoiseSchedule(timesteps=TIMESTEPS, device=DEVICE)\n\n    model = MediumUNet(in_ch=4, base_ch=BASE_CH, time_emb_dim=128).to(DEVICE)\n    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n    \n    # GradScaler is required when using Mixed Precision training to prevent gradient underflow\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n    ema_obj = EMA(model, decay=0.9999)\n\n    print('Starting training...')\n    train(model, schedule, loader, opt, scaler, ema_obj=ema_obj)\n\n    # Visualization Block\n    print('Generating samples...')\n    plt.figure(figsize=(8, 2 * SAMPLES_TO_SAVE))\n    \n    for i in range(SAMPLES_TO_SAVE):\n        x_gray, _ = dataset[i]\n        # Reshape to [1, 1, H, W] for the model\n        x_gray_b = x_gray.unsqueeze(0).to(DEVICE)\n        \n        # Inference using the trained model\n        y_sample = ddim_sample(model, x_gray_b, schedule, steps=50, eta=0.0, use_ema=True, ema_obj=ema_obj)[0].cpu()\n\n        # Display Input (Gray)\n        plt.subplot(SAMPLES_TO_SAVE, 2, i*2+1)\n        # Permute changes order from [Channel, Height, Width] to [Height, Width, Channel] for Matplotlib\n        plt.imshow(x_gray.permute(1,2,0).squeeze(), cmap='gray')\n        plt.axis('off')\n        \n        # Display Output (Color)\n        plt.subplot(SAMPLES_TO_SAVE, 2, i*2+2)\n        plt.imshow(y_sample.permute(1,2,0))\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Final Save\n    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'model_final.pth'))\n    torch.save(ema_obj.ema.state_dict(), os.path.join(CHECKPOINT_DIR, 'ema_final.pth'))\n    print('Saved final models.')\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}