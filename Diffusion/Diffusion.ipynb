{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# FULL 128x128 DDPM TRAINING PIPELINE\n# Features: Mixed Precision, Gradient Accumulation, EMA, and Robust Data Loading.\n\nprint(\"ðŸ”¥ Script started â€” loading imports...\")\n\n# Handle truncated/corrupt images in the dataset to prevent crashes\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport os\nimport random\nimport math\nimport copy\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# ---------------- CONFIGURATION ----------------\nDATASET_PATH = \"/kaggle/input/cartoon-classification/cartoon_classification/TRAIN\"\nIMG_SIZE = 128\nBASE_CH = 32              # UNet base channels\nBATCH_SIZE = 2            # Low batch size per GPU step to save VRAM\nACCUM_STEPS = 4           # effective_batch = BATCH_SIZE * ACCUM_STEPS\nEPOCHS = 12\nLR = 2e-4\nNUM_WORKERS = 4\nPIN_MEMORY = True\nTIMESTEPS = 1000          # Diffusion steps\nSUBSET_SIZE = None        # Set int to limit dataset size for testing\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nCHECKPOINT_DIR = \"checkpoints\"\nCHECKPOINT_FREQ = 1\nSAMPLES_TO_SAVE = 4\nSEED = 42\n\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(f\"Config: Device={DEVICE} | Size={IMG_SIZE} | Batch={BATCH_SIZE}x{ACCUM_STEPS}\")\n\n# ---------------- DATASET ----------------\nclass CartoonColorizationDataset(Dataset):\n    def __init__(self, root, img_size=IMG_SIZE, subset=SUBSET_SIZE):\n        self.paths = []\n        for r, d, f in os.walk(root):\n            for x in f:\n                if x.lower().endswith((\"jpg\",\"jpeg\",\"png\")):\n                    self.paths.append(os.path.join(r, x))\n        \n        self.paths.sort()\n        if subset:\n            self.paths = self.paths[:subset]\n            \n        self.tf = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()\n        ])\n        self.gray = transforms.Grayscale()\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        \n        # Robust loading for corrupt images\n        try:\n            img = Image.open(p).convert('RGB')\n        except Exception:\n            # Return gray placeholder if image is unreadable\n            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color=(128,128,128))\n\n        try:\n            y = self.tf(img)               # Ground truth (Color)\n            x = self.tf(self.gray(img))    # Condition (Grayscale)\n        except Exception:\n            y = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n            x = torch.zeros(1, IMG_SIZE, IMG_SIZE)\n\n        return x, y\n\n# ---------------- NOISE SCHEDULE ----------------\ndef linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n    return torch.linspace(beta_start, beta_end, timesteps)\n\nclass NoiseSchedule:\n    def __init__(self, timesteps=TIMESTEPS, device='cpu'):\n        self.timesteps = timesteps\n        betas = linear_beta_schedule(timesteps).to(device)\n        alphas = 1.0 - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), alphas_cumprod[:-1]], dim=0)\n\n        self.betas = betas\n        self.alphas = alphas\n        self.alphas_cumprod = alphas_cumprod\n        self.alphas_cumprod_prev = alphas_cumprod_prev\n\n        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n\n    def q_sample(self, x_start, t, noise=None):\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        \n        a = self.sqrt_alphas_cumprod[t].view(-1,1,1,1)\n        b = self.sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n        \n        return a * x_start + b * noise, noise\n\n# ---------------- MODEL (UNET) ----------------\nclass SinusoidalPositionEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        half = self.dim // 2\n        emb = math.log(10000) / (half - 1)\n        emb = torch.exp(torch.arange(half, device=t.device) * -emb)\n        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n        if self.dim % 2 == 1:\n            emb = torch.cat([emb, torch.zeros(t.size(0),1,device=t.device)], dim=1)\n        return emb\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.GroupNorm(8, out_ch) if out_ch>=8 else nn.BatchNorm2d(out_ch),\n            nn.SiLU(),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.GroupNorm(8, out_ch) if out_ch>=8 else nn.BatchNorm2d(out_ch),\n            nn.SiLU()\n        )\n        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch!=out_ch else nn.Identity()\n\n    def forward(self, x, t_emb=None):\n        h = self.net(x)\n        if t_emb is not None:\n            te = t_emb.unsqueeze(-1).unsqueeze(-1)\n            # Project emb if channels don't match\n            if te.shape[1] != h.shape[1]:\n                proj = nn.Linear(te.shape[1], h.shape[1]).to(te.device)\n                te = proj(t_emb).unsqueeze(-1).unsqueeze(-1)\n            h = h + te\n        return h + self.res_conv(x)\n\nclass MediumUNet(nn.Module):\n    def __init__(self, in_ch=4, base_ch=BASE_CH, time_emb_dim=128):\n        super().__init__()\n        # Time Embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmb(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim*4),\n            nn.SiLU(),\n            nn.Linear(time_emb_dim*4, time_emb_dim)\n        )\n\n        # Encoder\n        self.enc1 = ConvBlock(in_ch, base_ch)\n        self.down1 = nn.Conv2d(base_ch, base_ch*2, 4, stride=2, padding=1)\n        self.enc2 = ConvBlock(base_ch*2, base_ch*2)\n        self.down2 = nn.Conv2d(base_ch*2, base_ch*4, 4, stride=2, padding=1)\n        self.enc3 = ConvBlock(base_ch*4, base_ch*4)\n        self.down3 = nn.Conv2d(base_ch*4, base_ch*8, 4, stride=2, padding=1)\n\n        # Bottleneck\n        self.bot1 = ConvBlock(base_ch*8, base_ch*8)\n        self.bot2 = ConvBlock(base_ch*8, base_ch*8)\n\n        # Decoder\n        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 4, stride=2, padding=1)\n        self.dec3 = ConvBlock(base_ch*8, base_ch*4)\n        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 4, stride=2, padding=1)\n        self.dec2 = ConvBlock(base_ch*4, base_ch*2)\n        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 4, stride=2, padding=1)\n        self.dec1 = ConvBlock(base_ch*2, base_ch)\n\n        self.out = nn.Conv2d(base_ch, 3, 3, padding=1)\n\n    def forward(self, x_gray, y_noisy, t):\n        t_emb = self.time_mlp(t)\n        # Condition: Concatenate noisy image + grayscale input\n        x = torch.cat([y_noisy, x_gray], dim=1)\n\n        # Down\n        e1 = self.enc1(x, t_emb)\n        d1 = self.enc2(self.down1(e1), t_emb)\n        e2 = d1\n        d2 = self.enc3(self.down2(e2), t_emb)\n        e3 = d2\n        d3 = self.bot1(self.down3(e3), t_emb)\n        d3 = self.bot2(d3, t_emb)\n\n        # Up\n        u3 = self.up3(d3)\n        c3 = torch.cat([u3, e3], dim=1)\n        u3 = self.dec3(c3, t_emb)\n\n        u2 = self.up2(u3)\n        c2 = torch.cat([u2, e2], dim=1)\n        u2 = self.dec2(c2, t_emb)\n\n        u1 = self.up1(u2)\n        c1 = torch.cat([u1, e1], dim=1)\n        u1 = self.dec1(c1, t_emb)\n\n        return self.out(u1)\n\n# ---------------- EMA & SAMPLER ----------------\nclass EMA:\n    def __init__(self, model, decay=0.9999):\n        self.ema = copy.deepcopy(model).eval()\n        self.decay = decay\n        self.num_updates = 0\n\n    def update(self, model):\n        self.num_updates += 1\n        decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n        msd = model.state_dict()\n        for k, v in self.ema.state_dict().items():\n            if v.dtype.is_floating_point:\n                v *= decay\n                v += (1 - decay) * msd[k].detach()\n            else:\n                v.copy_(msd[k])\n\n@torch.no_grad()\ndef ddim_sample(model, x_gray, schedule, steps=50, eta=0.0, use_ema=False, ema_obj=None):\n    \"\"\"Samples using DDIM to speed up inference (e.g., 50 steps instead of 1000).\"\"\"\n    device = x_gray.device\n    B = x_gray.size(0)\n    T = schedule.timesteps\n    times = torch.linspace(T-1, 0, steps).long().to(device)\n\n    y = torch.randn(B,3,IMG_SIZE,IMG_SIZE, device=device)\n\n    if use_ema and ema_obj:\n        model_backup = copy.deepcopy(model.state_dict())\n        model.load_state_dict(ema_obj.ema.state_dict())\n\n    for i, t in enumerate(times):\n        t_batch = torch.full((B,), int(t.item()), dtype=torch.long, device=device)\n        eps_pred = model(x_gray, y, t_batch)\n\n        alpha_t = schedule.alphas_cumprod[t].to(device)\n        alpha_prev = schedule.alphas_cumprod_prev[t].to(device)\n        sqrt_alpha_t = torch.sqrt(alpha_t)\n        sqrt_alpha_prev = torch.sqrt(alpha_prev)\n\n        x0_pred = (y - torch.sqrt(1 - alpha_t) * eps_pred) / (sqrt_alpha_t + 1e-8)\n        sigma_t = eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t/alpha_prev + 1e-8))\n        dir_xt = torch.sqrt(torch.clamp(1 - alpha_prev - sigma_t ** 2, min=0.0)) * eps_pred\n        y = sqrt_alpha_prev * x0_pred + dir_xt\n\n    if use_ema and ema_obj:\n        model.load_state_dict(model_backup)\n\n    return y.clamp(0,1)\n\n# ---------------- TRAINING LOOP ----------------\ndef train(model, schedule, loader, opt, scaler, ema_obj=None):\n    model.train()\n    global_step = 0\n    \n    for epoch in range(EPOCHS):\n        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        running_loss = 0.0\n        opt.zero_grad()\n        \n        for step, (x_gray, y) in enumerate(pbar):\n            try:\n                x_gray = x_gray.to(DEVICE, non_blocking=PIN_MEMORY)\n                y = y.to(DEVICE, non_blocking=PIN_MEMORY)\n                \n                # Sample noise\n                B = y.size(0)\n                t = torch.randint(0, schedule.timesteps, (B,), device=DEVICE)\n                y_noisy, noise = schedule.q_sample(y, t)\n\n                # Mixed Precision Forward\n                with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n                    pred = model(x_gray, y_noisy, t)\n                    loss = ((pred - noise)**2).mean() / ACCUM_STEPS\n\n                # Backward & Step\n                scaler.scale(loss).backward()\n\n                if (step + 1) % ACCUM_STEPS == 0:\n                    scaler.step(opt)\n                    scaler.update()\n                    opt.zero_grad()\n                    if ema_obj:\n                        ema_obj.update(model)\n\n                running_loss += float(loss.item()) * ACCUM_STEPS\n                global_step += 1\n                pbar.set_postfix({'loss': running_loss / (global_step)})\n\n            except RuntimeError as e:\n                # Clear cache on OOM and skip batch\n                if 'out of memory' in str(e):\n                    print('OOM: Clearing cache...')\n                    torch.cuda.empty_cache()\n                    opt.zero_grad()\n                    continue\n                else:\n                    raise\n\n        # Save checkpoints\n        if (epoch + 1) % CHECKPOINT_FREQ == 0:\n            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'model_epoch{epoch+1}.pth'))\n            if ema_obj:\n                torch.save(ema_obj.ema.state_dict(), os.path.join(CHECKPOINT_DIR, f'ema_epoch{epoch+1}.pth'))\n\n    print('Training complete.')\n\n# ---------------- MAIN ----------------\ndef main():\n    dataset = CartoonColorizationDataset(DATASET_PATH, img_size=IMG_SIZE, subset=SUBSET_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n    print(f'Total images: {len(dataset)}')\n\n    schedule = NoiseSchedule(timesteps=TIMESTEPS, device=DEVICE)\n\n    model = MediumUNet(in_ch=4, base_ch=BASE_CH, time_emb_dim=128).to(DEVICE)\n    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n    ema_obj = EMA(model, decay=0.9999)\n\n    print('Starting training...')\n    train(model, schedule, loader, opt, scaler, ema_obj=ema_obj)\n\n    # Validate/Visuzalize\n    print('Generating samples...')\n    plt.figure(figsize=(8, 2 * SAMPLES_TO_SAVE))\n    \n    for i in range(SAMPLES_TO_SAVE):\n        x_gray, _ = dataset[i]\n        x_gray_b = x_gray.unsqueeze(0).to(DEVICE)\n        \n        y_sample = ddim_sample(model, x_gray_b, schedule, steps=50, eta=0.0, use_ema=True, ema_obj=ema_obj)[0].cpu()\n\n        plt.subplot(SAMPLES_TO_SAVE, 2, i*2+1)\n        plt.imshow(x_gray.permute(1,2,0).squeeze(), cmap='gray')\n        plt.axis('off')\n        \n        plt.subplot(SAMPLES_TO_SAVE, 2, i*2+2)\n        plt.imshow(y_sample.permute(1,2,0))\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Final Save\n    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'model_final.pth'))\n    torch.save(ema_obj.ema.state_dict(), os.path.join(CHECKPOINT_DIR, 'ema_final.pth'))\n    print('Saved final models.')\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}