{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2508873,"sourceType":"datasetVersion","datasetId":1519412}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install timm\nimport os\nimport glob\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom skimage import color\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport timm\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CartoonColorizationDataset(Dataset):\n    def __init__(self, root_dir, split='TRAIN', image_size=256):\n        # Recursively find all images in the TRAIN or TEST folders\n        self.image_paths = glob.glob(os.path.join(root_dir, 'cartoon_classification', split, '**', '*.jpg')) + \\\n                           glob.glob(os.path.join(root_dir, 'cartoon_classification', split, '**', '*.png'))\n        self.image_size = image_size\n        self.transforms = T.Compose([\n            T.Resize((image_size, image_size)),\n            T.ToTensor(), # Converts to [0, 1]\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        \n        # Open image and convert to RGB (ensure 3 channels)\n        img = Image.open(img_path).convert(\"RGB\")\n        img = img.resize((self.image_size, self.image_size), Image.Resampling.BILINEAR)\n        img_np = np.array(img)\n\n        # RGB to Lab conversion\n        # Lab values: L [0, 100], a [-128, 127], b [-128, 127] approximately\n        img_lab = color.rgb2lab(img_np)\n        \n        # Normalize L to [0, 1] for input\n        img_l = img_lab[:, :, 0] / 100.0 \n        \n        # Normalize ab to [-1, 1] range for stability (approximate)\n        img_ab = img_lab[:, :, 1:] / 128.0 \n\n        # Convert to Tensor [C, H, W]\n        img_l = torch.from_numpy(img_l).unsqueeze(0).float() # [1, 256, 256]\n        img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float() # [2, 256, 256]\n\n        return img_l, img_ab\n\n# Usage on Kaggle\nDATA_PATH = \"/kaggle/input/cartoon-classification\"\ntrain_dataset = CartoonColorizationDataset(DATA_PATH, split='TRAIN')\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_preprocessed_samples(dataset, num_samples=3):\n    \"\"\"\n    Visualizes L (Input) and reconstructed RGB (Ground Truth) from the dataset.\n    \"\"\"\n    # Create a figure with 2 rows (Input, Ground Truth) and 'num_samples' columns\n    fig, axes = plt.subplots(2, num_samples, figsize=(12, 8))\n    \n    # Randomly select indices\n    indices = np.random.choice(len(dataset), num_samples, replace=False)\n    \n    for i, idx in enumerate(indices):\n        l_tensor, ab_tensor = dataset[idx]\n        \n        # --- Process L Channel (Input) ---\n        # Shape: [1, H, W] -> [H, W]\n        l_img = l_tensor.squeeze().numpy() \n        # Display grayscale requires no conversion, just correct cmap\n        axes[0, i].imshow(l_img, cmap='gray')\n        axes[0, i].set_title(f\"Input (L Channel)\\nSample {idx}\")\n        axes[0, i].axis('off')\n\n        # --- Process Lab to RGB (Ground Truth) ---\n        # 1. Denormalize\n        l_orig = l_img * 100.0\n        ab_orig = ab_tensor.numpy() * 128.0 # Shape: [2, H, W]\n        \n        # 2. Stack to create Lab image [H, W, 3]\n        lab_img = np.zeros((256, 256, 3))\n        lab_img[:, :, 0] = l_orig\n        lab_img[:, :, 1:] = ab_orig.transpose(1, 2, 0)\n        \n        # 3. Convert Lab -> RGB using skimage\n        # Note: lab2rgb expects float64, and output is [0, 1]\n        rgb_img = color.lab2rgb(lab_img)\n        \n        axes[1, i].imshow(rgb_img)\n        axes[1, i].set_title(f\"Ground Truth (Recombined)\\nSample {idx}\")\n        axes[1, i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# --- Run the visualization ---\n# Make sure 'train_dataset' is defined from the previous step\nvisualize_preprocessed_samples(train_dataset, num_samples=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Backbone(nn.Module):\n    def __init__(self, model_name='convnext_tiny'):\n        super().__init__()\n        # features_only=True returns intermediate feature maps\n        self.model = timm.create_model(model_name, pretrained=True, features_only=True, in_chans=3)\n        \n        # Determine channel counts for the stages we need\n        # ConvNeXt usually outputs stride 4, 8, 16, 32\n        dummy = torch.randn(1, 3, 256, 256)\n        feats = self.model(dummy)\n        self.chans = [f.shape[1] for f in feats] \n        # e.g., tiny: [96, 192, 384, 768]\n    \n    def forward(self, x):\n        # Input x is Grayscale [B, 1, H, W]. ConvNeXt expects RGB.\n        # We repeat the grayscale channel 3 times.\n        x = x.repeat(1, 3, 1, 1)\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PixelDecoderBlock(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        # PixelShuffle upscales by 2x. Input channels must be out_c * (2^2)\n        self.conv = nn.Conv2d(in_c, out_c * 4, kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.act = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        return self.act(self.pixel_shuffle(self.conv(x)))\n\nclass PixelDecoder(nn.Module):\n    def __init__(self, encoder_chans, dec_chans=[256, 128, 64]):\n        super().__init__()\n        self.dec_chans = dec_chans\n        \n        # Define upsampling blocks adapting to encoder channels\n        # Note: This is a simplified UNet-style integration\n        self.up4 = PixelDecoderBlock(encoder_chans[3], dec_chans[0]) \n        self.up3 = PixelDecoderBlock(dec_chans[0] + encoder_chans[2], dec_chans[1])\n        self.up2 = PixelDecoderBlock(dec_chans[1] + encoder_chans[1], dec_chans[2])\n        self.up1 = PixelDecoderBlock(dec_chans[2] + encoder_chans[0], dec_chans[2]) # Keep dim\n        \n        # Final projection to Image Embedding Ei [C, H, W]\n        self.final = nn.Conv2d(dec_chans[2], 256, kernel_size=1) \n\n    def forward(self, feats):\n        # feats: [f0(1/4), f1(1/8), f2(1/16), f3(1/32)]\n        x = self.up4(feats[3]) \n        \n        # Resize to match skip connection if necessary (handling odd dims)\n        if x.shape != feats[2].shape: x = F.interpolate(x, size=feats[2].shape[2:])\n        x = torch.cat([x, feats[2]], dim=1)\n        x = self.up3(x)\n\n        if x.shape != feats[1].shape: x = F.interpolate(x, size=feats[1].shape[2:])\n        x = torch.cat([x, feats[1]], dim=1)\n        x = self.up2(x)\n        \n        if x.shape != feats[0].shape: x = F.interpolate(x, size=feats[0].shape[2:])\n        x = torch.cat([x, feats[0]], dim=1)\n        x = self.up1(x)\n        \n        return self.final(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ColorDecoder(nn.Module):\n    def __init__(self, in_channels, num_queries=100, hidden_dim=256, n_heads=4):\n        super().__init__()\n        self.num_queries = num_queries\n        \n        # 1. Projection layer to match Backbone channels to Transformer dim\n        self.projection = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n        \n        # 2. Learnable color queries\n        self.queries = nn.Parameter(torch.zeros(1, num_queries, hidden_dim))\n        \n        # 3. Transformer Decoder\n        self.transformer_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim*4, batch_first=True\n        )\n        self.transformer = nn.TransformerDecoder(self.transformer_layer, num_layers=3)\n        \n    def forward(self, img_features):\n        # img_features: [B, 768, H/32, W/32]\n        \n        # Project channels: [B, 768, H, W] -> [B, 256, H, W]\n        img_features = self.projection(img_features)\n        \n        # Flatten spatial dims: [B, 256, H, W] -> [B, 256, H*W] -> [B, H*W, 256]\n        B, C, H, W = img_features.shape\n        memory = img_features.view(B, C, -1).permute(0, 2, 1) \n        \n        # Expand queries: [1, 100, 256] -> [B, 100, 256]\n        queries = self.queries.repeat(B, 1, 1) \n        \n        # Apply Transformer\n        out = self.transformer(queries, memory) \n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DDColor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = Backbone()\n        \n        # Pixel Decoder (Handles upsampling)\n        self.pixel_decoder = PixelDecoder(self.backbone.chans)\n        \n        # Color Decoder (Handles color queries)\n        # We pass the number of channels from the last backbone layer (e.g., 768)\n        self.color_decoder = ColorDecoder(in_channels=self.backbone.chans[-1])\n        \n        # Fusion Layer\n        self.fusion_conv = nn.Conv2d(100, 2, kernel_size=1)\n        \n    def forward(self, x_l):\n        # 1. Backbone features\n        feats = self.backbone(x_l)\n        \n        # 2. Pixel Decoder -> [B, 256, H, W]\n        img_emb = self.pixel_decoder(feats)\n        \n        # 3. Color Decoder -> [B, 100, 256]\n        # Uses the deepest feature map (feats[-1])\n        color_queries = self.color_decoder(feats[-1]) \n        \n        # 4. Fusion (Dot Product Attention)\n        B, C, H, W = img_emb.shape\n        # Flatten image embeddings: [B, 256, H*W]\n        img_emb_flat = img_emb.view(B, C, -1)\n        \n        # Matrix Multiplication: \n        # [B, 100, 256] x [B, 256, H*W] -> [B, 100, H*W]\n        attention_map = torch.bmm(color_queries, img_emb_flat) \n        \n        # Reshape back to spatial dimensions\n        attention_map = attention_map.view(B, 100, H, W)\n        \n        # Final prediction convolution\n        out_ab = self.fusion_conv(attention_map) \n        \n        # Upsample to original resolution\n        out_ab = F.interpolate(out_ab, size=(256, 256), mode='bilinear')\n        return out_ab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm  # Automatically selects notebook-friendly widget on Kaggle\n\n# Hyperparameters\nEPOCHS = 6\nLR = 1e-4\n\n# Re-initialize model/optimizer to be safe\nmodel = DDColor().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\ncriterion = nn.L1Loss() \n\ndef train_one_epoch(model, loader, optimizer, criterion, epoch, total_epochs):\n    model.train()\n    total_loss = 0\n    \n    # Initialize tqdm progress bar\n    # leave=True keeps the bar after completion, leave=False clears it\n    loop = tqdm(loader, desc=f\"Epoch [{epoch+1}/{total_epochs}]\", leave=True)\n    \n    for l_img, ab_gt in loop:\n        l_img, ab_gt = l_img.to(device), ab_gt.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        ab_pred = model(l_img)\n        \n        # Calculate Loss\n        loss = criterion(ab_pred, ab_gt)\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Update the progress bar with the current loss\n        loop.set_postfix(loss=loss.item())\n        \n    return total_loss / len(loader)\n\n# --- Main Training Loop ---\nhistory = []\n\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, epoch, EPOCHS)\n    history.append(avg_loss)\n    \n    # Print epoch summary (tqdm handles the real-time bar)\n    print(f\"Epoch {epoch+1} Complete. Average Loss: {avg_loss:.5f}\")\n    \n    # Save checkpoint every 5 epochs\n    if (epoch+1) % 5 == 0:\n        torch.save(model.state_dict(), f'ddcolor_cartoon_ep{epoch+1}.pth')\n        print(f\"Checkpoint saved: ddcolor_cartoon_ep{epoch+1}.pth\")\n\nprint(\"Training Complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}